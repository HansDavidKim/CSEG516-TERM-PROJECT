import typer
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from pathlib import Path
from typing import List
import torchvision.utils as vutils
from PIL import Image

from classifier.models import ResNet152, VGG16, FaceNet
from dataloader import get_dataloader

app = typer.Typer()

@app.command()
def visualize(
    eval_model_checkpoint: str = typer.Option(..., "--eval-model", help="Path to evaluation classifier checkpoint"),
    data_root: str = typer.Option("dataset/private/celeba", "--data-root", help="Path to dataset root for real images"),
    results_dir: str = typer.Option("evaluation_results", "--results-dir", help="Directory containing attack results (images)"),
    model_name: str = typer.Option("ResNet152", "--model-name", help="Evaluation model architecture"),
    num_classes: int = typer.Option(1000, "--num-classes", help="Number of classes"),
    classes: str = typer.Option("0-9", "--classes", help="Classes to visualize"),
    device: str = typer.Option(None, "--device", help="Device override"),
    output_file: str = typer.Option("tsne_plot.png", "--output", help="Output filename for t-SNE plot"),
):
    """
    Generate t-SNE plot and comparison grid for RLB-MI results.
    """
    # Device setup
    if device:
        device_obj = torch.device(device)
    elif torch.cuda.is_available():
        device_obj = torch.device("cuda")
    elif torch.backends.mps.is_available():
        device_obj = torch.device("mps")
    else:
        device_obj = torch.device("cpu")
        
    print(f"Using device: {device_obj}")

    # Parse classes
    if '-' in classes:
        start, end = map(int, classes.split('-'))
        target_classes = list(range(start, end + 1))
    else:
        target_classes = [int(c) for c in classes.split(',')]

    # Load Evaluation Model (for feature extraction)
    print(f"Loading {model_name}...")
    if model_name == "VGG16":
        model = VGG16(num_classes)
    elif model_name == "ResNet152":
        model = ResNet152(num_classes)
    elif model_name in {"FaceNet", "Face.evoLVe"}:
        model = FaceNet(num_classes)
    else:
        raise ValueError(f"Unknown model: {model_name}")
        
    ckpt = torch.load(eval_model_checkpoint, map_location=device_obj)
    model.load_state_dict(ckpt['model'])
    model.to(device_obj)
    model.eval()

    # Collect Features
    features = []
    labels = []
    markers = [] # 'o' for real, 'x' for fake

    # 1. Real Images
    print("Extracting features from Real Images...")
    loader = get_dataloader(data_root, batch_size=32, resize_size=64, split='test')
    
    class_images_real = {c: [] for c in target_classes}
    max_real = 50 # Limit per class for plot clarity
    
    for images, lbls in loader:
        if lbls.dim() > 1:
            lbls = lbls.argmax(dim=1)
            
        for img, lbl in zip(images, lbls):
            l = lbl.item()
            if l in target_classes and len(class_images_real[l]) < max_real:
                class_images_real[l].append(img)
                
        if all(len(imgs) >= max_real for imgs in class_images_real.values()):
            break
            
    for c in target_classes:
        if not class_images_real[c]:
            continue
        
        imgs = torch.stack(class_images_real[c]).to(device_obj)
        with torch.no_grad():
            feats, _ = model(imgs)
            features.append(feats.cpu().numpy())
            labels.extend([c] * len(feats))
            markers.extend(['Real'] * len(feats))

    # 2. Reconstructed Images
    print("Extracting features from Reconstructed Images...")
    results_path = Path(results_dir)
    
    for c in target_classes:
        # Look for images in results_dir/class_{c}_images/ or similar
        # Based on evaluate_attack.py, it saves to output_dir (which is results_dir here)
        # But evaluate_attack.py didn't explicitly save individual images in the loop shown above?
        # Wait, I missed adding the image saving part in evaluate_attack.py loop!
        # I should check evaluate_attack.py again. 
        # Ah, I only saved the CSV. I should probably update evaluate_attack.py to save images too 
        # OR assume they are already generated by `run_rlb_mi_attack`.
        # Let's assume they are in `results_dir/class_{c}_images/*.png` as per `run_rlb_mi_attack`.
        # If `evaluate_attack.py` was used, I might need to update it to save images.
        
        # Let's check if I saved images in evaluate_attack.py.
        # I commented: "# Save images for this class (optional, maybe just a few)"
        # So I didn't save them. I should fix evaluate_attack.py first or just assume we run this after `run_rlb_mi_attack`.
        # But `evaluate_attack.py` is meant to run the attack. So it SHOULD save images.
        
        # I will proceed with writing this script assuming images are there, 
        # and then I will update evaluate_attack.py to actually save them.
        
        img_dir = results_path / f"class_{c}_images"
        if not img_dir.exists():
            print(f"Warning: Directory {img_dir} not found. Skipping class {c} reconstruction.")
            continue
            
        recon_imgs = []
        for img_file in img_dir.glob("*.png"):
            img = Image.open(img_file).convert('RGB')
            # Transform to tensor
            # Assuming standard normalization
            import torchvision.transforms as T
            transform = T.Compose([
                T.Resize((64, 64)),
                T.ToTensor(),
                T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
            ])
            recon_imgs.append(transform(img))
            if len(recon_imgs) >= max_real: # Limit to same number
                break
        
        if recon_imgs:
            imgs = torch.stack(recon_imgs).to(device_obj)
            with torch.no_grad():
                feats, _ = model(imgs)
                features.append(feats.cpu().numpy())
                labels.extend([c] * len(feats))
                markers.extend(['Reconstructed'] * len(feats))

    if not features:
        print("No features extracted. Exiting.")
        return

    features = np.concatenate(features, axis=0)
    labels = np.array(labels)
    markers = np.array(markers)

    # t-SNE
    print("Running t-SNE...")
    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(features)-1))
    embedded = tsne.fit_transform(features)

    # Plot
    print("Plotting...")
    plt.figure(figsize=(12, 10))
    
    # Define colors for classes
    unique_classes = np.unique(labels)
    colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_classes)))
    class_color_map = dict(zip(unique_classes, colors))
    
    for i in range(len(features)):
        c = labels[i]
        m = markers[i]
        
        marker_style = 'o' if m == 'Real' else 'x'
        alpha = 0.6 if m == 'Real' else 1.0
        size = 50 if m == 'Real' else 100
        
        plt.scatter(
            embedded[i, 0], embedded[i, 1],
            c=[class_color_map[c]],
            marker=marker_style,
            alpha=alpha,
            s=size,
            label=f"Class {c} {m}" if f"Class {c} {m}" not in plt.gca().get_legend_handles_labels()[1] else ""
        )

    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.title(f"t-SNE Visualization of Real vs Reconstructed Images ({model_name} Features)")
    plt.tight_layout()
    plt.savefig(output_file)
    print(f"Plot saved to {output_file}")

if __name__ == "__main__":
    app()
